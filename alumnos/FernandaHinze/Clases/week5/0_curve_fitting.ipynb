{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajuste de curvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interact, fixed, widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El problema clásico de ajuste de curvas es dado un conjunto de puntos, encontrar la curva que **mejor** representa los datos. Donde **mejor** está sujeto a definición."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacerlo más divertido, verémos el problema de ajuste de curvas como un problema de optimización y en particular como un método de aprendizaje de máquina (_machine learning_) supervisado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que tenemos un conjunto $m$ de datos de entrada $\\textbf{x}$ (en aprendizaje de máquina se les conoce como _features_) y sus datos dependientes, $\\textbf{y}$ (_target_ en aprendizaje de máquina). A este conjunto $(x^{(i)}, y^{(i)})$, le llamamos conjunto de entrenamiento. Queremos desarrollar un modelo $\\hat{\\textbf{y}}$ que aproxime el valor de $\\textbf{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera aproximación es intentar una aproximación de forma lineal, conocida como _regresión lineal_, de la forma\n",
    "\n",
    "$$\n",
    "\\hat{\\textbf{y}} = \\mathbf\\beta_0 + \\mathbf\\beta_1 \\textbf{x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donde (si recuerdan Geometría Analítica) $\\beta_0$ es el _interceptor_ de la recta $\\hat{\\textbf{y}}$ y $\\beta_1$ es la _pendiente_ de la recta. A $\\hat{\\textbf{y}}$ se le conoce también como _hipótesis_ y se le puede denotar con la variable $\\textbf{h}$. Si definimos que $x_0 \\equiv 1$, podemos escribir la _hipótesis_ de manera más compacta:\n",
    "\n",
    "$$\n",
    "\\hat{\\textbf{y}} = \\sum_{j=0}^n \\beta_j x_j = \\mathbf{\\beta}^T\\textbf{x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esta notación, podemos extender el formalismo a más dimensiones (en este caso $n$).\n",
    "\n",
    "Una posible definición de **mejor** es que el modelo (el cual está determinado por $\\vec\\beta$) minimice la suma de las diferencias entre el valor actual $\\textbf{y}$ y el predicho $\\hat{\\textbf{y}}$ (a esta diferencia se le conoce como _error en la predicción_), en otras palabras _minimizar la suma del cuadrado de los residuos_. La función a minimizar se conoce en aprendizaje de máquina como _función de costo_ $\\textbf{J}$. Debido a que tenemos varios pares $(x_i, y_i)$, la función costo a minimizar es el _error cuadrático promedio_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\textbf{J}(\\beta_0, \\beta_1) = \\frac{1}{2n}\\sum_{(x^{(i)}, y^{(i)}) \\in X \\times Y} (y^{(i)} - \\hat{y}^{(i)}(x^{(i)}))^2 = \\frac{1}{2n}\\sum_{(x^{(i)}, y^{(i)}) \\in X \\times Y} (y^{(i)} - \\beta_0 - \\beta_1 x^{(i)})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El factor de $2$ se agrega para simplificar cálculos posteriores. Visto así, el objetivo de un algoritmo supervisado de aprendizaje de máquina es encontrar $\\beta_0$ y $\\beta_1$ que minimiza la función de costo $\\textbf{J}(\\beta_0, \\beta_1)$. Esto se puede hacer mediante un algoritmo llamado _gradient descent_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El gradiente de una función $g(x,y)$ es:\n",
    "\n",
    "$$\n",
    "\\nabla g (x, y) = \\left [ \\partial_x g, \\ \\partial_y g\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "donde $\\partial_x$, $\\partial_y$ es la derivada parcial respecto a $x$ y $y$ respectivamente. el significado geométrico del gradiente de una función, es el vector que apunta en la dirección donde se maximiza el incremento de la función. Por lo tanto, si queremos minimizar la función, recorremos el vector en el sentido contrario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Escoger un punto al azar, llama a este punto $\\boldsymbol{B}_0$.\n",
    "2. Calcular el gradiente de $\\textbf{J}$ en esa locación.\n",
    "3. Actualiza la locación en el sentido opuesto a donde apunte el gradiente, específicamente resta a $\\boldsymbol{B}_0$ el valor de $\\alpha\\nabla\\textbf{J}$, donde $\\alpha$ es un número pequeño, conocido como _learning rate_.\n",
    "4. Repite los pasos $2$ y $3$ cuantas veces sea necesario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En pseudocódigo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "_repetir hasta que lograr convergencia {_\n",
    "\n",
    "$$\\beta_j := \\beta_j - \\alpha \\frac{\\partial}{\\partial \\beta_j} \\textbf{J}(\\beta)$$\n",
    "\n",
    "_}_\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se deja como ejercicio de tarea, demostrar que \n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\beta_j} \\textbf{J}(\\beta) = \\frac{1}{m}\\sum_{i=1}^{m}\\left(\\hat{y}(x^{(i)}) - y(x^{(i)})\\right) \\cdot x^{(i)}_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "_repetir hasta que lograr convergencia {_\n",
    "\n",
    "$$\n",
    "\\beta_j := \\beta_j - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}\\left(\\hat{y}(x^{(i)}) - y(x^{(i)})\\right) \\cdot x^{(i)}_j\n",
    "$$\n",
    "\n",
    "_}_\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class RegresionLineal:\n",
    "    def __init__(self, alpha=0.3, max_iters=100, tols=0.001):\n",
    "        \"\"\"\n",
    "        Parámetros.\n",
    "        ---------------\n",
    "        alpha = Learning rate\n",
    "        max_iters = Número máximo de iteraciones\n",
    "        tols = definición de convergencia\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.max_iters = max_iters\n",
    "        self.tols = tols\n",
    "        self.breaking_iteration = None\n",
    "        self.historia = {'costo':[], 'beta':[]}  # Con fines de graficación\n",
    "        \n",
    "    def gradientDescent(self, x, y):\n",
    "        \"\"\"\n",
    "        Parámetros:\n",
    "        ---------------\n",
    "        x = vector de entrenamiento de features\n",
    "        y = vector de entrenamiento de variable a predecir (target)\n",
    "        \"\"\"    \n",
    "        \n",
    "        # ajustamos el vector de features\n",
    "        unos = np.ones((x.shape[0], 1))\n",
    "        Xt = X.reshape(x.shape[0], 1)\n",
    "        Xt = np.concatenate((unos, Xt), axis=1)\n",
    "        \n",
    "        i = 0\n",
    "        prep_J = 0\n",
    "        m, n = Xt.shape\n",
    "        self.beta = np.zeros(n) \n",
    "        \n",
    "        while i < self.max_iters:     \n",
    "            # Actualizamos beta\n",
    "            self.beta = self.beta - self.alpha * self.gradiente(Xt, y)\n",
    "            \n",
    "            J = self.costo(Xt, y)\n",
    "            \n",
    "            if abs(J - prep_J) <= self.tols:\n",
    "                print('La función convergió con beta: %s en la iteración %i' % ( str(self.beta), i ))\n",
    "                self.breaking_iteration = i\n",
    "                break\n",
    "            else:\n",
    "                prep_J = J\n",
    "            \n",
    "            self.historia['costo'].append(J)\n",
    "            self.historia['beta'].append(self.beta)                \n",
    "            i += 1\n",
    "    \n",
    "    def hipotesis(self, x):\n",
    "        return np.dot(x, self.beta)\n",
    "    \n",
    "    def costo(self, x, y):\n",
    "        m = x.shape[0]\n",
    "        error = self.hipotesis(x) - y\n",
    "        return np.dot(error.T, error) / (2 * m) \n",
    "    \n",
    "    def gradiente(self, x, y):\n",
    "        m = x.shape[0]\n",
    "        error = self.hipotesis(x) - y        \n",
    "        return np.dot(x.T, error) / m    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los archivos `edad.dat` y `altura.dat` contienen las mediciones de las estaturas (en metros) de varios niños entre las edad de 2 y 8 años. Cada _tupla_ de altura y edad, constituyen un ejemplo de entrenamiento $(x^{(i)}, y^{(i)})$ de nuestros datos. Hay $m = 50$ datos para entrenar que usaremos para realizar un modelo de regresión lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.loadtxt('data/edad.dat')\n",
    "Y = np.loadtxt('data/altura.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos se ven así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X,Y, label=\"data\")\n",
    "plt.xlabel('Edad (yr)')\n",
    "plt.ylabel('Altura (m)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Ejercicio:** Usando el _widget_ siguiente, trata de encontrar al \"tanteo\" cuál es la recta que mejor minimiza el error.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotGuess(x, y, interceptor, pendiente):\n",
    "    \n",
    "    modelo = lambda x,b,m: b + m*x # función para graficar el modelo\n",
    "    \n",
    "    plt.scatter(X,Y, label=\"data\")\n",
    "    plt.plot(X, modelo(X, interceptor, pendiente), label='Guess')\n",
    "    plt.xlabel('Edad (yr)')\n",
    "    plt.ylabel('Altura (m)')\n",
    "    plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(plotGuess, x=fixed(X), y=fixed(Y), interceptor=(0,2,0.02), pendiente=(0,2, 0.02));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos la regresión lineal con un _learning rate_ de $\\alpha = 0.03$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = RegresionLineal(alpha=0.03, max_iters=10000, tols=0.0000001)\n",
    "r.gradientDescent(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "**Ejercicio: ** Agrega un _widget_ de interacción en el cual puedas modificar $\\alpha$. ¿Qué observas?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotModelo(x,y,rl,iteracion):\n",
    "    modelo = lambda x,b,m: b + m*x # función para graficar el modelo\n",
    "    \n",
    "    _beta = rl.historia['beta'][iteracion]\n",
    "\n",
    "    fig, ax = plt.subplots(1,2, figsize=(10,6))\n",
    "    ax[0].scatter(x,y, label=\"datos\")\n",
    "    ax[0].plot(x, modelo(x, _beta[0], _beta[1]), label=\"int: %1.2f, pen: %1.2f\" % (_beta[0], _beta[1]))\n",
    "    ax[0].set_xlabel('Edad (yr)')\n",
    "    ax[0].set_ylabel('Altura (m)')\n",
    "    ax[0].legend(loc=\"best\")\n",
    "    #ax[0].set_xlim(0, max(x))\n",
    "    #ax[0].set_ylim(0, max(y))\n",
    "    \n",
    "    costo  = rl.historia['costo']\n",
    "    \n",
    "    iteraciones = [i for i in range(0, len(costo))]\n",
    "    ax[1].plot(iteraciones, costo, 'g', label=\"costo\")\n",
    "    ax[1].plot(iteracion, costo[iteracion], 'or', label=\"iteracion\")\n",
    "    ax[1].set_xlabel('Iteraciones')\n",
    "    ax[1].set_ylabel('Costo')\n",
    "    ax[1].legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotModelo(X,Y, r, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ultima_iteracion = (r.breaking_iteration - 1) if r.breaking_iteration else (r.max_iters - 1)\n",
    "\n",
    "interact(plotModelo, x=fixed(X), y=fixed(Y), rl=fixed(r), iteracion=(0,ultima_iteracion,10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "**Ejercicio**: Modifica el _widget_ que creaste, para que dibujes la última iteración ¿Tienes una mejor intuición de $\\alpha$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Una cosa más..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "A pesar de que la regresión lineal (obtenida con el _gradient descent_) parece un algoritmo muy simple, los conceptos son los mismos que para algoritmos de aprendizaje de máquina más avanzados, i.e. minimizar una función de costo. Estos algoritmos simplemente reemplazan el modelo linear con un modelo más complejo (y con una función de costo más compleja). De cierta manera, los algoritmos de aprendizaje de máquina son problemas de optimización.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
